{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strands SDK: Max Tokens Handling Explained\n",
    "\n",
    "This notebook demonstrates how the Strands SDK handles the `max_tokens` limit when it's reached during model responses. We'll walk through the complete flow step-by-step.\n",
    "\n",
    "## âœ… What You'll Learn\n",
    "\n",
    "1. How to set up an agent with token limits\n",
    "2. What happens when max_tokens is reached\n",
    "3. The message recovery process\n",
    "4. How the agent handles incomplete tool calls\n",
    "5. Agent recovery after exceptions\n",
    "\n",
    "## âŒ Key Behavior\n",
    "\n",
    "When `max_tokens` is reached, Strands SDK **fails fast** rather than continuing with potentially corrupted responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from strands import Agent, tool\n",
    "from strands.models.bedrock import BedrockModel\n",
    "from strands.types.exceptions import MaxTokensReachedException\n",
    "\n",
    "# Enable logging to see internal recovery process\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ… Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tools\n",
    "\n",
    "We'll create a tool that would typically generate a long response, making it likely to hit token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tools defined!\n",
      "ğŸ“‹ Available tools: story_tool, weather_tool\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def story_tool(story: str) -> str:\n",
    "    \"\"\"\n",
    "    Tool that writes a story that is minimum 50,000 lines long.\n",
    "    This tool is designed to trigger max_tokens in our example.\n",
    "    \"\"\"\n",
    "    return story\n",
    "\n",
    "@tool\n",
    "def weather_tool(city: str) -> str:\n",
    "    \"\"\"Get weather information for a city.\"\"\"\n",
    "    return f\"The weather in {city} is sunny and 75Â°F.\"\n",
    "\n",
    "print(\"âœ… Tools defined!\")\n",
    "print(f\"ğŸ“‹ Available tools: story_tool, weather_tool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Agent with Low Token Limit\n",
    "\n",
    "We'll create an agent with `max_tokens=100` - a very low limit that will easily be exceeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Agent created!\n",
      "ğŸ”¢ Token limit: 100\n",
      "ğŸ› ï¸ Available tools: ['story_tool', 'weather_tool']\n",
      "ğŸ“Š Initial message count: 0\n"
     ]
    }
   ],
   "source": [
    "# Create model with very low token limit\n",
    "model = BedrockModel(max_tokens=100)\n",
    "agent = Agent(model=model, tools=[story_tool, weather_tool])\n",
    "\n",
    "print(f\"âœ… Agent created!\")\n",
    "print(f\"ğŸ”¢ Token limit: {model.config.get('max_tokens')}\")\n",
    "print(f\"ğŸ› ï¸ Available tools: {list(agent.tool_registry.registry.keys())}\")\n",
    "print(f\"ğŸ“Š Initial message count: {len(agent.messages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Trigger Max Tokens Exception\n",
    "\n",
    "Let's make a request that will likely exceed our 100-token limit. The model will try to:\n",
    "1. Generate a text response\n",
    "2. Call the `story_tool` \n",
    "3. Get cut off mid-generation due to token limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Making request: 'Tell me a story!'\n",
      "ğŸ“ Expected behavior:\n",
      "   1. Model starts generating response\n",
      "   2. Model attempts to call story_tool\n",
      "   3. Response gets truncated at 100 tokens\n",
      "   4. Tool call becomes incomplete\n",
      "   5. MaxTokensReachedException is raised\n",
      "\n",
      "I'd be happy to tell you a story! Let me create one for you.\n",
      "Tool #1: story_tool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:strands.event_loop._recover_message_on_max_tokens_reached:handling max_tokens stop reason - replacing all tool uses with error messages\n",
      "WARNING:strands.event_loop._recover_message_on_max_tokens_reached:tool_name=<story_tool> | replacing with error message due to max_tokens truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MaxTokensReachedException caught as expected!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸš€ Making request: 'Tell me a story!'\")\n",
    "print(\"ğŸ“ Expected behavior:\")\n",
    "print(\"   1. Model starts generating response\")\n",
    "print(\"   2. Model attempts to call story_tool\")\n",
    "print(\"   3. Response gets truncated at 100 tokens\")\n",
    "print(\"   4. Tool call becomes incomplete\")\n",
    "print(\"   5. MaxTokensReachedException is raised\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    result = agent(\"Tell me a story!\")\n",
    "    print(\"âŒ Unexpected: No exception was thrown!\")\n",
    "    print(f\"Result: {result.stop_reason}\")\n",
    "    \n",
    "except MaxTokensReachedException as e:\n",
    "    print(\"âœ… MaxTokensReachedException caught as expected!\")\n",
    "    # print(f\"ğŸ“„ Exception message: {str(e)}\")\n",
    "    # print()\n",
    "    # print(\"ğŸ” This exception indicates:\")\n",
    "    # print(\"   - The model response was truncated\")\n",
    "    # print(\"   - Any tool calls were incomplete/corrupted\")\n",
    "    # print(\"   - The agent is in an unrecoverable state for this request\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Examine Message Recovery\n",
    "\n",
    "Before the exception was thrown, the SDK automatically recovered the message by:\n",
    "1. Preserving all text content\n",
    "2. Replacing incomplete tool calls with error messages\n",
    "3. Saving the cleaned message to conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Conversation History After Max Tokens:\n",
      "ğŸ“Š Total messages: 2\n",
      "\n",
      "ğŸ’¬ Message 1 (user):\n",
      "   ğŸ“„ Text content 1: Tell me a story!\n",
      "\n",
      "ğŸ’¬ Message 2 (assistant):\n",
      "   ğŸ“„ Text content 1: I'd be happy to tell you a story! Let me create one for you.\n",
      "   ğŸ“„ Text content 2: The selected tool story_tool's tool use was incomplete due to maximum token limits being reached.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“‹ Conversation History After Max Tokens:\")\n",
    "print(f\"ğŸ“Š Total messages: {len(agent.messages)}\")\n",
    "print()\n",
    "\n",
    "for i, message in enumerate(agent.messages):\n",
    "    print(f\"ğŸ’¬ Message {i+1} ({message['role']}):\")\n",
    "    \n",
    "    for j, content in enumerate(message.get('content', [])):\n",
    "        if 'text' in content:\n",
    "            text = content['text']\n",
    "            # Truncate long text for display\n",
    "            # display_text = text if len(text) <= 150 else text[:150] + \"...\"\n",
    "            display_text = text\n",
    "            print(f\"   ğŸ“„ Text content {j+1}: {display_text}\")\n",
    "            \n",
    "        elif 'toolUse' in content:\n",
    "            tool_use = content['toolUse']\n",
    "            print(f\"   ğŸ”§ Tool use {j+1}: {tool_use}\")\n",
    "            \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inspect the complete conversation flow\n",
    "# def inspect_message_flow(messages):\n",
    "#     print(\"=== DETAILED MESSAGE FLOW ===\")\n",
    "    \n",
    "#     for i, message in enumerate(messages):\n",
    "#         print(f\"\\n--- Message {i+1} ---\")\n",
    "#         print(f\"Role: {message['role']}\")\n",
    "        \n",
    "#         for j, content in enumerate(message['content']):\n",
    "#             print(f\"  Content {j+1}:\")\n",
    "            \n",
    "#             if 'text' in content:\n",
    "#                 text = content['text']\n",
    "#                 # Truncate long text for readability\n",
    "#                 # if len(text) > 200:\n",
    "#                 #     text = text[:200] + \"...\"\n",
    "#                 print(f\"    Text: {text}\")\n",
    "            \n",
    "#             elif 'toolUse' in content:\n",
    "#                 tool_use = content['toolUse']\n",
    "#                 print(f\"    Tool Use: {tool_use['name']}\")\n",
    "#                 print(f\"    Input: {tool_use['input']}\")\n",
    "#                 print(f\"    ID: {tool_use['toolUseId']}\")\n",
    "            \n",
    "#             elif 'toolResult' in content:\n",
    "#                 tool_result = content['toolResult']\n",
    "#                 print(f\"    Tool Result: {tool_result['status']}\")\n",
    "#                 print(f\"    ID: {tool_result['toolUseId']}\")\n",
    "#                 # Don't print full content as it's very long\n",
    "#                 print(f\"    Content: [Raw KB Response - {len(str(tool_result['content']))} chars]\")\n",
    "\n",
    "# # Run the inspection\n",
    "# inspect_message_flow(agent.messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': [{'text': 'Tell me a story!'}]},\n",
       " {'content': [{'text': \"I'd be happy to tell you a story! Let me create one for you.\"},\n",
       "   {'text': \"The selected tool story_tool's tool use was incomplete due to maximum token limits being reached.\"}],\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Verify Recovery Error Message\n",
    "\n",
    "Let's check if the expected recovery error message was inserted by the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Recovery Process Verification:\n",
      "   Expected error text: 'tool use was incomplete due to maximum token limits being reached'\n",
      "   âœ… Recovery message found: True\n",
      "\n",
      "ğŸ’¡ This confirms the SDK successfully:\n",
      "   1. Detected incomplete tool use\n",
      "   2. Replaced it with an informative error message\n",
      "   3. Preserved the conversation context\n"
     ]
    }
   ],
   "source": [
    "# Look for the specific error message that indicates tool recovery\n",
    "expected_error_text = \"tool use was incomplete due to maximum token limits being reached\"\n",
    "\n",
    "# Extract all text content from messages\n",
    "all_text_content = [\n",
    "    content[\"text\"]\n",
    "    for message in agent.messages\n",
    "    for content in message.get(\"content\", [])\n",
    "    if \"text\" in content\n",
    "]\n",
    "\n",
    "# Check if recovery message exists\n",
    "has_recovery_message = any(expected_error_text in text for text in all_text_content)\n",
    "\n",
    "print(f\"ğŸ” Recovery Process Verification:\")\n",
    "print(f\"   Expected error text: '{expected_error_text}'\")\n",
    "print(f\"   âœ… Recovery message found: {has_recovery_message}\")\n",
    "print()\n",
    "\n",
    "if has_recovery_message:\n",
    "    print(\"ğŸ’¡ This confirms the SDK successfully:\")\n",
    "    print(\"   1. Detected incomplete tool use\")\n",
    "    print(\"   2. Replaced it with an informative error message\")\n",
    "    print(\"   3. Preserved the conversation context\")\n",
    "else:\n",
    "    print(\"âŒ Recovery message not found - unexpected behavior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Agent Recovery\n",
    "\n",
    "Even though we hit max_tokens, the agent should still be functional for new requests. Let's test this by:\n",
    "1. Removing tools to avoid tool-related token usage\n",
    "2. Making a simple request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Testing Agent Recovery:\n",
      "   Removing tools to avoid tool-related token usage...\n",
      "   ğŸ› ï¸ Tools cleared. Remaining tools: []\n",
      "\n",
      "ğŸ§® Making simple request: 'What is 3+3?'\n",
      "3 + 3 = 6âœ… Recovery successful!\n",
      "ğŸ“Š Stop reason: end_turn\n",
      "ğŸ“„ Response: 3 + 3 = 6\n",
      "\n",
      "ğŸ’¡ This demonstrates:\n",
      "   - Agent remains functional after MaxTokensReachedException\n",
      "   - Previous conversation history is preserved\n",
      "   - New requests are processed normally\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”„ Testing Agent Recovery:\")\n",
    "print(\"   Removing tools to avoid tool-related token usage...\")\n",
    "\n",
    "# Clear tools to prevent tool use in recovery test\n",
    "original_registry = agent.tool_registry.registry.copy()\n",
    "original_config = agent.tool_registry.tool_config.copy() if agent.tool_registry.tool_config is not None else None\n",
    "\n",
    "agent.tool_registry.registry = {}\n",
    "agent.tool_registry.tool_config = {}\n",
    "\n",
    "print(f\"   ğŸ› ï¸ Tools cleared. Remaining tools: {list(agent.tool_registry.registry.keys())}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    print(\"ğŸ§® Making simple request: 'What is 3+3?'\")\n",
    "    recovery_result = agent(\"What is 3+3?\")\n",
    "    \n",
    "    print(\"âœ… Recovery successful!\")\n",
    "    print(f\"ğŸ“Š Stop reason: {recovery_result.stop_reason}\")\n",
    "    print(f\"ğŸ“„ Response: {recovery_result.message['content'][0]['text']}\")\n",
    "    print()\n",
    "    print(\"ğŸ’¡ This demonstrates:\")\n",
    "    print(\"   - Agent remains functional after MaxTokensReachedException\")\n",
    "    print(\"   - Previous conversation history is preserved\")\n",
    "    print(\"   - New requests are processed normally\")\n",
    "    \n",
    "except Exception as recovery_error:\n",
    "    print(f\"âŒ Recovery failed: {recovery_error}\")\n",
    "    print(\"This would indicate a more serious issue with agent state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Final Conversation State\n",
    "\n",
    "Let's examine the complete conversation after recovery to see how everything was preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Final Conversation State:\n",
      "ğŸ“Š Total messages: 4\n",
      "\n",
      "ğŸ’¬ Message 1 (user):\n",
      "   ğŸ“„ Tell me a story!\n",
      "\n",
      "ğŸ’¬ Message 2 (assistant):\n",
      "   ğŸ“„ I'd be happy to tell you a story! Let me create one for you.\n",
      "   ğŸ“„ The selected tool story_tool's tool use was incomplete due to maximum token limits being reached.\n",
      "\n",
      "ğŸ’¬ Message 3 (user):\n",
      "   ğŸ“„ What is 3+3?\n",
      "\n",
      "ğŸ’¬ Message 4 (assistant):\n",
      "   ğŸ“„ 3 + 3 = 6\n",
      "\n",
      "ğŸ› ï¸ Tools restored: ['story_tool', 'weather_tool']\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“‹ Final Conversation State:\")\n",
    "print(f\"ğŸ“Š Total messages: {len(agent.messages)}\")\n",
    "print()\n",
    "\n",
    "for i, message in enumerate(agent.messages):\n",
    "    print(f\"ğŸ’¬ Message {i+1} ({message['role']}):\")\n",
    "    \n",
    "    for j, content in enumerate(message.get('content', [])):\n",
    "        if 'text' in content:\n",
    "            text = content['text']\n",
    "            # Show first and last part of long text\n",
    "            if len(text) > 100:\n",
    "                display_text = text[:50] + \"...\" + text[-50:]\n",
    "            else:\n",
    "                display_text = text\n",
    "            print(f\"   ğŸ“„ {display_text}\")\n",
    "            \n",
    "    print()\n",
    "\n",
    "# Restore tools for future use\n",
    "agent.tool_registry.registry = original_registry\n",
    "agent.tool_registry.tool_config = original_config\n",
    "print(f\"ğŸ› ï¸ Tools restored: {list(agent.tool_registry.registry.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: How Strands SDK Handles Max Tokens\n",
    "\n",
    "### ğŸ”„ The Complete Flow:\n",
    "\n",
    "1. **Request Processing** (`src/strands/agent/agent.py:377`)\n",
    "   - Agent receives user prompt\n",
    "   - Calls `event_loop_cycle()` to process request\n",
    "\n",
    "2. **Model Invocation** (`src/strands/event_loop/event_loop.py:144`)\n",
    "   - Model generates response with `max_tokens=100` limit\n",
    "   - Response gets truncated when limit is reached\n",
    "   - Returns `stop_reason=\"max_tokens\"`\n",
    "\n",
    "3. **Message Recovery** (`src/strands/event_loop/event_loop.py:162`)\n",
    "   - `recover_message_on_max_tokens_reached()` is called\n",
    "   - Preserves all text content\n",
    "   - Replaces incomplete tool calls with error messages\n",
    "   - Returns cleaned message\n",
    "\n",
    "4. **Exception Handling** (`src/strands/event_loop/event_loop.py:221`)\n",
    "   - Cleaned message is added to conversation history\n",
    "   - `MaxTokensReachedException` is raised\n",
    "   - Agent state is preserved but request terminates\n",
    "\n",
    "### âœ… What's Preserved:\n",
    "- Conversation history with cleaned messages\n",
    "- Agent configuration and state\n",
    "- Text content from model responses\n",
    "\n",
    "### âŒ What's Prevented:\n",
    "- Execution of incomplete/corrupted tool calls\n",
    "- Continuation with potentially invalid state\n",
    "- Silent failures or unexpected behavior\n",
    "\n",
    "### ğŸ”§ Design Philosophy:\n",
    "**Fail Fast, Preserve State** - Rather than attempting to continue with potentially corrupted data, the SDK fails immediately while preserving conversation context for recovery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Example: Multiple Tool Calls\n",
    "\n",
    "Let's see what happens when max_tokens is reached during multiple tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh agent for this test\n",
    "model_multi = BedrockModel(max_tokens=150)  # Slightly higher to see partial progress\n",
    "agent_multi = Agent(model=model_multi, tools=[story_tool, weather_tool])\n",
    "\n",
    "print(\"ğŸ”¬ Advanced Test: Multiple Tool Calls with Max Tokens\")\n",
    "print(f\"ğŸ”¢ Token limit: {model_multi.config.get('max_tokens')}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # Request that might trigger multiple tool calls\n",
    "    result = agent_multi(\"Tell me a story about the weather in New York, then get the actual weather there\")\n",
    "    print(\"âŒ No exception - request completed within token limit\")\n",
    "    print(f\"Stop reason: {result.stop_reason}\")\n",
    "    \n",
    "except MaxTokensReachedException as e:\n",
    "    print(\"âœ… MaxTokensReachedException during multi-tool scenario\")\n",
    "    print()\n",
    "    \n",
    "    # Analyze what tool calls were affected\n",
    "    tool_recovery_messages = []\n",
    "    for message in agent_multi.messages:\n",
    "        for content in message.get('content', []):\n",
    "            if 'text' in content and 'tool use was incomplete' in content['text']:\n",
    "                tool_recovery_messages.append(content['text'])\n",
    "    \n",
    "    print(f\"ğŸ”§ Tool recovery messages found: {len(tool_recovery_messages)}\")\n",
    "    for i, msg in enumerate(tool_recovery_messages):\n",
    "        print(f\"   {i+1}. {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### âœ… **Do:**\n",
    "- Set appropriate `max_tokens` for your use case\n",
    "- Handle `MaxTokensReachedException` in your application\n",
    "- Trust that conversation state is preserved\n",
    "- Continue using the agent after exceptions\n",
    "\n",
    "### âŒ **Don't:**\n",
    "- Assume tool calls will complete when near token limits\n",
    "- Ignore `MaxTokensReachedException` \n",
    "- Set `max_tokens` too low for complex tasks\n",
    "- Expect partial tool execution\n",
    "\n",
    "### ğŸ¯ **Best Practices:**\n",
    "1. **Monitor token usage** in your applications\n",
    "2. **Set generous limits** for tool-heavy workflows\n",
    "3. **Implement retry logic** with higher limits if needed\n",
    "4. **Log exceptions** for debugging and optimization\n",
    "\n",
    "The Strands SDK's approach ensures **reliability and predictability** even when hitting resource limits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-on-aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
